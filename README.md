# Weather MCP Application

A complete implementation of the **Model Context Protocol (MCP)** with Python 3, featuring separate server and client modules for weather data interaction, including interactive CLI and LLM-powered clients.

## ğŸŒŸ What is MCP?

The Model Context Protocol (MCP) is an open standard developed by Anthropic for connecting AI applications with external data sources and tools. Think of MCP like USB-C for AI - it provides a standardized way to:

- **Expose Resources**: Data that can be read by AI models
- **Provide Tools**: Functions that AI models can execute
- **Define Prompts**: Reusable templates for AI interactions

## ğŸ“ Project Structure

```
weather-app-mcp/
â”œâ”€â”€ server/                       # MCP Server Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mcp_server.py            # Weather server implementation
â”œâ”€â”€ client/                       # MCP Client Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mcp_client.py            # Core MCP client implementation
â”‚   â”œâ”€â”€ interactive_client.py    # Simple interactive CLI
â”‚   â””â”€â”€ llm_interactive_client.py # LLM-powered interactive client
â”œâ”€â”€ examples/                     # Example scripts
â”‚   â”œâ”€â”€ run_server.py            # Run server standalone
â”‚   â”œâ”€â”€ run_client.py            # Run client demo
â”‚   â””â”€â”€ custom_client.py         # Custom client examples
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_server.py
â”‚   â”œâ”€â”€ test_client.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ pyproject.toml                # Package configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ INTERACTIVE_GUIDE.md          # Interactive clients usage guide
â”œâ”€â”€ INTERACTIVE_CLIENTS_SUMMARY.md # Technical details
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)

### Installation

1. **Clone the repository** (or navigate to your project directory):
```bash
cd weather-app-mcp
```

2. **Create a virtual environment** (recommended):
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### ğŸ¯ Interactive Clients

The project includes **two interactive clients** that accept user queries and respond in real-time:

#### 1. Simple Interactive CLI Client

A command-line interface for querying weather data with simple commands:

```bash
python client/interactive_client.py
```

**Available commands:**
- `weather <city>` - Get current weather for a city
- `compare <city1> <city2>` - Compare weather between two cities
- `summary` - Get temperature summary for all cities
- `resource <city>` - Read raw weather resource data (JSON)
- `list` - Show all available cities
- `help` - Show help message
- `quit` or `exit` - Exit the client

**Available cities**: `new_york`, `london`, `tokyo`, `paris`

**Example session:**
```
weather> weather london
Current weather in London:
Temperature: 58Â°F
Conditions: Rainy
Humidity: 80%
Wind Speed: 15 mph

weather> compare new_york tokyo
Weather Comparison:

New York: 72Â°F, Partly Cloudy
Tokyo: 68Â°F, Clear

Temperature difference: 4Â°F (New York is warmer)

weather> summary
Temperature Summary Across All Cities:

Average Temperature: 65.0Â°F
Hottest: New York at 72Â°F
Coldest: London at 58Â°F
Range: 14Â°F

weather> quit
Goodbye! ğŸ‘‹
```

#### 2. LLM-Powered Interactive Client

Use natural language queries with AI (OpenAI GPT, Anthropic Claude, or AWS Bedrock). The LLM client automatically understands your questions, selects the right tools, and provides conversational responses.

**Prerequisites:**

1. **Install LLM dependencies:**

Edit `requirements.txt` and uncomment the LLM provider you want to use:
```python
# Uncomment one of these:
openai>=1.0.0              # For OpenAI GPT integration
# anthropic>=0.18.0        # For Anthropic Claude integration
# boto3>=1.28.0            # For AWS Bedrock integration
```

Then install:
```bash
pip install -r requirements.txt
```

Or install directly:
```bash
pip install openai           # For OpenAI
# or
pip install anthropic        # For Anthropic Claude
# or
pip install boto3            # For AWS Bedrock
```

2. **Set up API credentials:**

**For OpenAI:**
```bash
export OPENAI_API_KEY='sk-your-key-here'
export LLM_PROVIDER=openai  # Optional, this is the default
```

**For Anthropic Claude:**
```bash
export ANTHROPIC_API_KEY='sk-ant-your-key-here'
export LLM_PROVIDER=anthropic
```

**For AWS Bedrock:**
```bash
export AWS_ACCESS_KEY_ID='your-access-key'
export AWS_SECRET_ACCESS_KEY='your-secret-key'
export AWS_REGION='us-east-1'  # Optional, defaults to us-east-1
export LLM_PROVIDER=bedrock

# Optional: Specify a different Bedrock model
export BEDROCK_MODEL_ID='anthropic.claude-3-5-sonnet-20241022-v2:0'
```

Or use AWS CLI configuration:
```bash
aws configure
export LLM_PROVIDER=bedrock
```

3. **Run the client:**
```bash
python client/llm_interactive_client.py
```

**Example natural language queries:**
```
You: What's the weather like in London?

ğŸ¤” Thinking...
ğŸ¤– Calling tool: get_current_weather with args: {'city': 'london'}